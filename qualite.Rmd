---
title: "Les mesures de qualité d'un modèle"
site: distill::distill_website
---

Après vous avoir introduit la classification et les techniques de classement, il convient de développer un peu notre compréhension de ces dernières.

Les techniques de classement sont de deux types :

-   Les techniques inductives ;
-   Les techniques transductives.

Les techniques transductives étant beaucoup moins utilisées que les techniques inductives nous ne les développerons pas ici.

Les techniques de classement inductives nécessitent une phase d'apprentissage, permettant de construire un modèle, qui résume les relations entre les variables, et qui peut de nouveau être utilisé sur de nouvelles données pour prédire un classement.

Ce classement se déroule en quatre étapes :

-   Une étape d'apprentissage, qui sera effectuée à partir d'un échantillon d'individus, tiré aléatoirement, dont on connaît le classement, parmi la population à modéliser ;

-   Une étape de test. L'idée est de tester le modèle sur un autre échantillon d'individus dont on connaît déjà le classement et qui sont aléatoirement tirés de la même population que l'échantillon d'apprentissage.

-   Une étape de validation

-   Une étape d'application

## Les qualités attendues

### La précision

Dans une **technique de classement**, on cherche à ce que le taux d'erreur, autrement dit la proportion d'individus mal classés, soit le plus bas possible. Plusieurs indicateurs de qualité sont à notre disposition comme **la courbe ROC**, que nous aurons l'occasion d'explorer par la suite.

### La robustesse

Il faut faire attention à ce que notre modèle dépende le moins possible de l'échantillon particulier d'apprentissage utilisé afin qu'il puisse se généraliser sur d'autres échantillons. Notre modèle doit donc être le moins sensible possible aux fluctuations aléatoires de certaines variables. Le modèle doit pouvoir continuer à s'appliquer à de nouveaux échantillons pendant un certains laps de temps. Ce laps de temps dépend, bien sûr, du type de données.

### La parcimonie

Les règles du modèle doivent idéalement être aussi simples et peu nombreuses que possible. Elles seront ainsi faciles à comprendre, à analyser, à contrôler et à généraliser sur d'autres échantillons. Il faut savoir que la concision peut être facteur de robustesse.

### Des résultats explicites

La forme des résultats à une importance capitale, en effet, dans certaines industries, elle est même nécessaire à l'utilisation du modèle. Par exemple, dans le milieu médical, imaginons que nous construisions un modèle qui classe des patients et qui détermine une pathologie à partir des symptômes du patient. Il sera nécessaire de connaître les règles de décisions qui classent le patient ayant telle ou telle pathologie. Dans d'autres industries la forme aura moins d'importance, cependant il faudra tout de même que le modèle soit utilisable et facilement intégrable au système d'information de l'entreprise.

### Les données

Toutes les techniques ne sont pas forcément capables de manipuler certains types de données, qu'elles soient continues, discrètes, qualitatives ou ... manquantes.

### La rapidité de calcul

Ce point est ouvert à la discussion puisqu'il dépend fortement des modèles que l'on souhaite utiliser. En effet, les modèles les plus complexes sont, en général, les plus performants en termes de précision, robustesse... mais aussi les plus lents à construire. Il faut dans l'idéal être capable d'optimiser la vitesse d'apprentissage au maximum, afin de pouvoir procéder à des ajustements et à des tests sans passer sa vie à la machine à café en attendant que les modèles se construisent. Ce point dépend donc de la méthode de modélisation utilisée, de son implémentation informatique et de la puissance de calcul de notre machine.

### Les possibilités de paramétrage

Dans un classement, il peut être intéressant de pouvoir pondérer les erreurs de classement, pour signifier, par exemple, qu'il est plus grave de classer un patient malade comme "non-malade" que l'inverse. Selon les besoins de l'analyse, la variété de paramétrage disponible peut être très importante à prendre en compte.

## Mesure de la qualité

### Généralisation

La taille de l'échantillon d'apprentissage à une importance déterminante dans la qualité du modèle. Un petit échantillon permet d'obtenir un faible taux d'erreur en apprentissage. Cependant, le modèle se généralisera mal, faute d'un apprentissage complet, et par conséquent, l'erreur en test sera élevée.

Dans le cas inverse, la performance en apprentissage sera en apparence moins bonne, mais se généralisera mieux sur le test.

Il faut savoir que l'erreur en apprentissage ne croit pas indéfiniment avec la taille de l'échantillon d'apprentissage et l'erreur en test ne décroit pas indéfiniment non plus.

Il faudra donc réaliser un arbitrage pour déterminer la taille optimale d'échantillon d'apprentissage à choisir.

**Exemple :**

```{r graph généralisation,echo=FALSE,warning=FALSE}
library(ggplot2)
gris_mecen='#344a58'
bleu_mecen='#238694'
x<-seq(0,1000,1)
bonne_generalisation=c(0.48,0.57)
mauvaise_generalisation=c(0.30,0.75)

base <- ggplot() + xlim(0,50)+ylim(0,1)
base +
  geom_function(fun = function(x) 1/(x+2)+0.55,color=bleu_mecen)+
  geom_function(fun = function(x) -1/(x+2)+0.5,color="pink")+
  geom_line(aes(x=33,y=x),linetype="dotted",color=gris_mecen)+
  geom_line(aes(x=x,y=0.525),linetype="dashed",color=gris_mecen)+
  geom_line(aes(x=35,y=bonne_generalisation),color=gris_mecen)+
  geom_line(aes(x=2.5,y=mauvaise_generalisation),color=gris_mecen)+
  annotate(geom="text", x=20, y=0.65, label="Erreur de test",color=bleu_mecen)+
  annotate(geom="text", x=20, y=0.40, label="Erreur d'apprentissage",color="pink")+
  annotate(geom="text", x=10, y=0.55, label="Mauvaise généralisation",color=gris_mecen)+
  annotate(geom="text", x=42, y=0.55, label="Bonne généralisation",color=gris_mecen)+
  annotate(geom="text", x=38, y=0.05, label="Taille suffisante",color=gris_mecen)+
  ylab("Taux d'erreur")+
  xlab("Taille de l'échantillon")+ theme_classic()+
  theme(axis.ticks = element_blank(),
        axis.text = element_blank(),
        panel.background = element_blank(),
        legend.position = "top",
        plot.title = element_text(size=14, face="bold",hjust = 0.5),
        axis.title.x = element_text(size=12, face="bold"),
        axis.title.y = element_text(size=12, face="bold"),
        legend.text = element_text(size = 12))+
  ggtitle("Erreur en fonction de la taille de l'échantillon")
```

Cette figure nous montre où se trouve la taille minimale de l'échantillon d'apprentissage :

-   en dessous de laquelle, le modèle obtenu en apprentissage ne se généralisera pas aussi bien que possible ;
-   au dessus de laquelle, les erreurs de test et d'apprentissage ne varient plus sensiblement.

Pour construire un modèle suffisamment robuste, il faut une taille d'échantillon, par classe à prédire, conséquente. Au grand minimum quelques centaines d'individus.

### Validation croisée

Afin de rendre la phase d'apprentissage la plus robuste possible, le test peut être réalisé d'une autre façon : par validation croisée. Le principe est simple, on scinde la population en 10 échantillons aléatoires de tailles égales, le dixième échantillon sera l'échantillon de test et les neufs autres seront les échantillons d'apprentissage. On peut donc créer un modèle sur les échantillons d'apprentissage et obtenir un taux d'erreur de test. Nous pouvons répéter l'opération 9 nouvelles fois en changeant d'échantillon test à chaque fois. Finalement, nous pouvons regarder la moyenne des dix taux d'erreur obtenus des dix modèles partiels pour estimer l'erreur du modèle construit sur toutes les données.

+---------------+---------------+---------------+---------------+----------------+
| #### TEST     | Apprentissage | Apprentissage | Apprentissage | ...            |
+---------------+---------------+---------------+---------------+----------------+
| Apprentissage | #### TEST     | Apprentissage | ...           | Apprentissage  |
+---------------+---------------+---------------+---------------+----------------+
| Apprentissage | Apprentissage | #### ...      | Apprentissage | Apprentissage  |
+---------------+---------------+---------------+---------------+----------------+
| Apprentissage | ...           | Apprentissage | #### TEST     | Apprentissage  |
+---------------+---------------+---------------+---------------+----------------+
| ...           | Apprentissage | Apprentissage | Apprentissage | #### TEST      |
+---------------+---------------+---------------+---------------+----------------+
| #### Modèle 1 | #### Modèle 2 | #### ...      | #### Modèle 9 | #### Modèle 10 |
+---------------+---------------+---------------+---------------+----------------+

Prendre 10 échantillons est un choix fréquent, mais en théorie, on peut avoir autant d'échantillons que d'individus, où à chaque étape ce sera seulement un individu qui sera mis de côté. On appelle cela "leave-one-out", ce découpage permet d'être certain de la robustesse du modèle final mais pour des raisons évidentes de temps de calcul, il ne peut être utilisé que quand $n$ est petit.

### Sur-apprentissage, sous-apprentissage

Le **sur-apprentissage** est un phénomène qui arrive quand des modèles ont été trop poussés pendant la phase d'apprentissage et que les modèles résultant sont beaucoup trop complexes et épousent toutes les fluctuations de l'échantillon d'apprentissage. Ce phénomène est grave car il a pour conséquence de fausser complètement le modèle et les prédictions qui en découleront. Il est impossible de **généraliser** avec un modèle qui a fait du **sur-apprentissage**.

À l'inverse, un modèle trop simple peut faire ce qu'on appelle communément du **sous-apprentissage**, c'est-à-dire que nous n'avons pas encore atteint un niveau de complexité suffisant dans le modèle pour correctement modéliser telle ou telle relation. La conséquence du sous-apprentissage est un taux d'erreur élevé en apprentissage et en test.

**Exemple :**

```{r data pour graph sursous apprentissage, echo = F}
df1 <- data.frame(X= c(1,1.2,2,2.1,2.15,2.16,2.3,2.5,3,3.1,4,4.5,5.2,5.4,5.42), Y= c(5,1,2.5,4,6,4.5,4.2,5.3,4.9,4.2,5,3.2,5.5,5.9,5), Color = factor("Groupe 1"))
df2 <- data.frame(X= c(2.5,2.55,2.56,2.7,3,3.3,4,4.1,5.1,5.15,5.5,
                       3.5,4.1,5),
                  Y= c(2.6,2.7,3,4,3.2,3.5,3.7,3.1,4.1,4.15,3.5,
                       2.5,1.5,2),
                  Color = factor("Groupe 2"))
df3 <- data.frame(X= c(1.5,2.5,2.55,2.56,2.7,3,3.3,4,4.5,5.1,5.15,5.5,5.7), Y= c(1,2.9,3.1,3.2,4.3,3.4,3.7,3.9,3,4.3,4.4,3.7,3.7))
df4<-data.frame(X=c(1,5.5),Y=c(3,5))
x_bien<-seq(2.05,5.7,0.1)
y_bien<--1/(x_bien-1.8)+4.8
df5<-data.frame(X=x_bien,Y=y_bien)
```

```{r graph sursous apprentissage,echo=FALSE,warning=FALSE}
library(plotly)
updatemenus <- list(
  list(x=1.3,y=0.3,
    type="buttons",
    buttons=list(
      list(
        label="Sur-apprentissage",
        method="update",
        args=list(list(visible=c(T,T,T,F,F)),list(title="Sur-apprentissage"))
      ),
      list(
        label="Sous-apprentissage",
        method="update",
        args=list(list(visible=c(T,T,F,T,F)),list(title="Sous-apprentissage"))
      ),
      list(
        label="Bon modèle",
        method="update",
        args=list(list(visible=c(T,T,F,F,T)),list(title="Bon modèle"))
      ),
      list(
        label="Tous",
        method="update",
        args=list(list(visible=c(T,T,T,T,T)),list(title="Tous les modèles"))
      )
    )
  )
)
fig <- plot_ly() %>%
  add_markers(x=df1$X, y=df1$Y, marker=list(color=bleu_mecen), name = "Groupe 1") %>%
  add_markers(x=df2$X, y=df2$Y, marker=list(color='pink'), name = "Groupe 2")%>%
  add_lines(x=df3$X, y=df3$Y,visible=F,name='Point par point',color="#344a57")%>%
  add_lines(x=df4$X, y=df4$Y,visible=F,name='Linéaire',color="#344a58")%>%
  add_lines(x=df5$X, y=df5$Y,visible=F,name='Quadratique',color="#344a59")%>%
  layout(title = "Projection des groupes sur les différents axes", showlegend = TRUE,updatemenus = updatemenus,
         xaxis = list(tickvals = c(-1,0,1, 2,3,4,5,6,7)),
           yaxis = list(tickvals = c(-1,0,1, 2,3,4,5,6,7)))
fig
```

La figure ci-dessus montre bien l'arbitrage à réaliser entre les trois modèles. Un modèle en sur-apprentissage va avoir une variance forte qui lui sera associé, un modèle en sous-apprentissage va avoir un biais important, un bon modèle va avoir une variance faible et un petit biais.

```{r graph arbitrage sur sous,echo=FALSE,warning=FALSE}
library(ggplot2)
gris_mecen='#344a58'
bleu_mecen='#238694'
x<-seq(0,10,0.1)
bonne_generalisation=c(0.48,0.57)
mauvaise_generalisation=c(0.30,0.75)

base <- ggplot() +ylim(0,10)+xlim(0,10)
base +
  geom_function(fun = function(x) 1/(x/4)+1,color=bleu_mecen)+
  geom_function(fun = function(x) (x/1.5-3.5)^2+2.5,color="pink")+
  geom_line(aes(x=4,y=x),linetype="dotted",color=gris_mecen)+
  geom_line(aes(x=6,y=x),linetype="dotted",color=gris_mecen)+
  annotate(geom="text", x=8, y=2, label="Erreur d'apprentissage",color=bleu_mecen)+
  annotate(geom="text", x=7.7, y=8, label="Erreur de test",color="pink")+
  annotate(geom="text", x=2, y=0.5, label="Sous-apprentissage",color=gris_mecen)+
  annotate(geom="text", x=5, y=0.5, label="Bon modèle",color=gris_mecen)+
  annotate(geom="text", x=8, y=0.5, label="Sur-apprentissage",color=gris_mecen)+
  ylab("Taux d'erreur")+
  xlab("Complexité du modèle")+theme_classic()+
  theme(axis.ticks=element_blank(),
        axis.text = element_blank(),
        legend.position = "top",
        plot.title = element_text(size=14, face="bold",hjust = 0.5),
        axis.title.x = element_text(size=12, face="bold"),
        axis.title.y = element_text(size=12, face="bold"),
        legend.text = element_text(size = 12))+
  ggtitle("Erreur en fonction de la complexité du modèle")
```

**Attention !** Le sur-apprentissage n'est pas forcément uniquement dû à une trop grande complexité du modèle. Il peut être lié à un bon ajustement sur un échantillon biaisé, ou bien au fait qu'une variable explicative soit fortement corrélée avec la variable à expliquer.

### Phase de test : Évaluation de la performance

Peu importe à quel point notre échantillon d'apprentissage est bon, on sait qu'il ne peut à lui seul renseigner sur la capacité de généralisation du modèle à la population entière. L'échantillon d'apprentissage ne peut pas servir à la fois à l'élaboration du modèle et à la validation puisque cette dernière sera bien trop optimiste. Nous devons donc disposer d'un échantillon distinct pour tester les modèles construits. La taille de l'échantillon test est généralement plus petite que l'échantillon d'entraînement.

Pour un modèle de classement, la performance peut se traduire en une **matrice de confusion** :

+-------------+-------------+--------------+-----------+-------------+
|             |             | #### Réalisé |           |             |
+-------------+-------------+--------------+-----------+-------------+
|             |             | ##### Oui    | ##### Non | ##### Total |
+-------------+-------------+--------------+-----------+-------------+
| #### Prédit | ##### Oui   | 300          | 150       | 450         |
+-------------+-------------+--------------+-----------+-------------+
| ####        | ##### Non   | 50           | 500       | 550         |
+-------------+-------------+--------------+-----------+-------------+
| ####        | ##### Total | 350          | 650       | 1000        |
+-------------+-------------+--------------+-----------+-------------+

: Exemple d'une matrice de confusion

Cette matrice nous permet de mesurer le taux de mauvais classement ou le *taux d'erreur*. Par exemple, dans notre cas $(150+50)/1000=20\%$. On distingue plusieurs scores qui nous donnent différentes informations : *la précision, la sensibilité et la spécificité*. 

+ **La précision** : l'inverse du taux d'erreur soit les individus correctement prédits sur le total des individus soit $80\%$ dans notre cas ;

+ **La sensibilité** : ici on dira que les "Oui" sont considérés comme l'évènement à prédire, la sensibilité est donc le pourcentage d'évènements bien prédits, ici ce sera donc $300/450=66.67\%$ ;

+ **La spécificité** : ici on dira que les "Non" sont considérés comme le non-évènement, la spécificité est donc le pourcentage de non-évènements bien prédits, ici ce sera donc $500/550=90.90\%$.

**Pourquoi s'intéresser à la spécificité et la sensibilité et pas seulement à la précision du modèle ?**

Selon l'objectif de l'analyse, il est nécessaire d'ajuster les prédictions. On peut vouloir minimiser *l'erreur globale* du modèle ou maximiser *la précision* ; on peut aussi vouloir maximiser la sensibilité ; enfin on peut aussi vouloir maximiser la spécificité selon les besoins de l'analyse. 

Imaginons que nous travaillions sur des modèles de credit-scoring et que nous souhaitions prédire la probabilité qu'un individu rembourse son crédit. Ici, il sera préférable, par mesure de sécurité, de prédire plus d'individus dans la catégorie risquée alors qu'ils ne le sont pas, que l'inverse, soit prédire un individu comme ne présentant pas de risque de ne pas rembourser comme un individu qui remboursera son crédit. Ici, on souhaitera maximiser *la spécificité*.

Selon le besoin de l'analyse, il faudra donc réaliser un arbitrage entre *précision*, *sensibilité* et *spécificité*. Évidemment, l'idéal étant d'avoir les trois scores proches de 1.  

**Courbe ROC**

Pour faciliter cet arbitrage, les statisticiens ont recherché des critères universels de performance d'un modèle. Ces critères s'appliquent aux modèles de classement en deux classes. Dans le cas de trois classes, il n'y a pas de généralisation simple du critère dont nous allons parler.

On peut visualiser le pouvoir discriminant d'un modèle grâce à **la courbe ROC** (Receiver Operating Characteristic). Cette courbe nous permet de visualiser la proportion d'évènements correctement détectés (sensibilité) en fonction de la proportion de non-évènements mal détectés (1-Spécificité).   

```{r courbe ROC train,echo=FALSE,fig.height=6,fig.width=6}
library(ROCR)
library(ISLR)
library(MASS)
library(caret)
library(tidyverse)
library(rsample)
library(class)

df <- Default[,c(1,3,4)]
data_split <- df %>% initial_split(prop = 2/3)
test_data <- data_split %>% testing()
train_data <- data_split %>% training()

lda.fit <- lda(default~., data = train_data)
res.lda <- predict(lda.fit, newdata =test_data)
pred.lda <- prediction(res.lda$posterior[,2], test_data$default)
roc.lda <- performance(pred.lda, "tpr", "fpr")

qda.fit <- qda(default~., data = train_data)
res.qda <- predict(qda.fit, newdata =test_data)
pred.qda <- prediction(res.qda$posterior[,2], test_data$default)
roc.qda <- performance(pred.qda, "tpr", "fpr")

knn.fit <- knn.fit <- train(default ~ .,
                data = train_data,
                method = "knn",
                preProcess=c("center", "scale"))
res.knn <- predict(knn.fit,type="prob", newdata =test_data)
pred.knn <- prediction(res.knn[,2], test_data$default)
roc.knn <- performance(pred.knn, "tpr", "fpr")

ggplot()+geom_line(aes(x=unlist(roc.lda@x.values),y=unlist(roc.lda@y.values),color="red"))+
  geom_line(aes(x=unlist(roc.qda@x.values),y=unlist(roc.qda@y.values),color="green"))+
  geom_line(aes(x=unlist(roc.knn@x.values),y=unlist(roc.knn@y.values),color="blue"))+
  geom_line(aes(x=c(0,1),y=c(0,1)))+
  ylab("Taux de vrais positifs (Sensibilité)")+
  xlab("Taux de faux positifs (1-Spécificité)")+ theme_classic()+
  scale_color_manual("Modèle",values=c(bleu_mecen,"pink",gris_mecen),labels=c("LDA","QDA","KNN"))+
  theme(panel.background = element_blank(),
        legend.position = "top",
        plot.title = element_text(size=14, face="bold",hjust = 0.5),
        axis.title.x = element_text(size=12, face="bold"),
        axis.title.y = element_text(size=12, face="bold"),
        legend.text = element_text(size = 12))+
  ggtitle("Courbes ROC")

```

Si cette courbe coïncide avec la diagonale, le modèle n'est pas plus performant qu'un modèle aléatoire. À l'inverse, plus la courbe ROC est proche du coin en haut à gauche, meilleur est le modèle puisqu'il permettra de prédire des taux importants de vrais positifs et de vrais négatifs (Sensibilité proche de 1 et Spécificité-1 proche de 0). La courbe ROC d'un modèle parfait serait composée de deux segments reliant les coordonnées (0, 0) à (0, 1) et (0,1) à (1, 1).   

Globalement, on peut comparer les performances de deux modèles en comparant les aires sous leurs courbes ROC respectives. Par exemple le modèle ayant une courbe ROC qui coïncide avec la diagonale aura une aire sous la courbe de 0.5, ce qui correspond au score d'un modèle aléatoire. Le modèle parfait aura une aire sous la courbe de 1 et un modèle qui aurait une courbe ROC en dessous de la diagonale aurait une aire sous la courbe inférieure à 0.5, ce qui veut dire que ce modèle réaliserait des prédictions encore pires que des prédictions aléatoires. 

